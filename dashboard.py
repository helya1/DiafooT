import streamlit as st
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import cm
cm.get_cmap("coolwarm")

# ================================
# üåê Streamlit Page Setup
# ================================
st.set_page_config(page_title="DIAFOOT Analysis Dashboard", layout="wide")
st.title("üìä DIAFOOT Analysis Dashboard")

# ================================
# üì§ File Upload
# ================================
uploaded_file = st.file_uploader("Upload Excel file with 'DIAFOOT' sheet", type=["xlsx"])

if uploaded_file:
    df = pd.read_excel(uploaded_file, sheet_name="DIAFOOT", header=None)

    analysis_type = st.sidebar.radio(
        "üß™ Choose Analysis Type:",
        ("üìã Stat Summary Extractor", "üß© KMeans Clustering Based on Selected Features",
        "üß¨ ED Thickness & Hypodermis Ultrasound Analysis", "üß© GMM Clustering vs Grades", 
        "üß† Correlation Between Key Parameters", "üìå Mechanical Features Correlation Analysis")
    )

    # ================================
    # üßπ Data Preparation Function
    # ================================
    def prepare_data(df):
        label_risk = "Grade de risque IWGDF"
        row_risk = df[df[0].astype(str).str.strip().str.lower() == label_risk.lower()]
        if row_risk.empty:
            st.error(f"Label '{label_risk}' not found.")
            st.stop()

        idx_risk = row_risk.index[0]
        risk_values = pd.to_numeric(df.iloc[idx_risk, 1:], errors='coerce').dropna().astype(int)

        ed_data = df.iloc[126:134, 1:1 + len(risk_values)]
        ed_data.index = [
            "ED SESA R (mm)", "ED HALLUX R (mm)", "ED TM5 R (mm)", "ED Other R (mm)",
            "ED SESA L (mm)", "ED HALLUX L (mm)", "ED TM5 L (mm)", "ED Other L (mm)"
        ]
        df_ed = ed_data.T.apply(pd.to_numeric, errors='coerce')

        hypo_data = df.iloc[134:142, 1:1 + len(risk_values)]
        hypo_data.index = [
            "US Hypoderme SESA R (mm)", "US Hypoderme HALLUX R (mm)",
            "US Hypoderme TM5 R (mm)", "US Hypoderme Other R (mm)",
            "US Hypoderme SESA L (mm)", "US Hypoderme HALLUX L (mm)",
            "US Hypoderme TM5 L (mm)", "US Hypoderme Other L (mm)"
        ]
        df_hypo = hypo_data.T.apply(pd.to_numeric, errors='coerce')

        df_combined = pd.concat([df_ed, df_hypo], axis=1).dropna()
        df_combined["Grade"] = risk_values.loc[df_combined.index].values
        df_combined["Group"] = df_combined["Grade"].apply(lambda x: "A (Grades 0-1)" if x in [0, 1] else "B (Grades 2-3)")
        return df_combined

    # ================================
    # üìã Stat Summary Extractor
    # ================================
    if analysis_type == "üìã Stat Summary Extractor":
        target_rows = {
            17: "Taille (m)", 18: "Poids (kg)", 35: "MESI PRESSION GO D", 36: "MESI PRESSION GO G",
            37: "IPS GO D", 38: "IPS GO G", 94: "AMPLI MTP1 D", 95: "AMPLI MTP1 G",
            96: "AMPLI TALO CRURALE D", 97: "AMPLI TALO CRURALE G",
            108: "Pression MOYENNE DES MAX SESA D", 109: "Pression MOYENNE DES MAX HALLUX D",
            110: "Pression MOYENNE DES MAX  TM5 D", 113: "Pression MOYENNE DES MAX  SESA G",
            114: "Pression MOYENNE DES MAX  HALLUX G", 115: "Pression MOYENNE DES MAX  TM5 G",
            118: "DURO SESA D", 119: "DURO HALLUX D", 120: "DURO TM5 D", 122: "DURO SESA G",
            123: "DURO HALLUX G", 124: "DURO TM5 G", 142: "√âpaisseur TOTALE PARTIES MOLLES  SESA D",
            143: "√âpaisseur TOTALE PARTIES MOLLES  HALLUX D", 144: "√âpaisseur TOTALE PARTIES MOLLES  TM5 D",
            146: "√âpaisseur TOTALE PARTIES MOLLES  SESA G", 147: "√âpaisseur TOTALE PARTIES MOLLES  HALLUX G",
            148: "√âpaisseur TOTALE PARTIES MOLLES  TM5 G", 150: "ROC SESA D", 151: "ROC HALLUX D",
            152: "ROC TM5 D", 154: "ROC SESA G", 155: "ROC HALLUX G", 156: "ROC TM5 G",
            212: "SUDOSCAN main D", 213: "SUDOSCAN main G", 214: "SUDOSCAN pied D", 215: "SUDOSCAN pied G"
        }

        summary = []
        deviation_table = {}

        for row_index, label in target_rows.items():
            values = pd.to_numeric(df.iloc[row_index, 1:], errors='coerce').dropna()
            if values.empty:
                continue
            mean = values.mean()
            std_dev = values.std()
            deviation = values - mean

            summary.append({"Measure": label, "Mean": round(mean, 2), "Standard Deviation": round(std_dev, 2)})
            deviation_table[label] = deviation.round(2)

        summary_df = pd.DataFrame(summary)
        deviation_df = pd.DataFrame(deviation_table).dropna()
        deviation_df.index.name = "Patient Index"

        st.subheader("üìä Summary Table")
        st.dataframe(summary_df)

        st.subheader("üìâ Deviation from Mean (√âcart √† la Moyenne)")
        st.dataframe(deviation_df)

        output_filename = "DIAFOOT_stats_and_deviation.xlsx"
        with pd.ExcelWriter(output_filename) as writer:
            summary_df.to_excel(writer, sheet_name="Summary", index=False)
            deviation_df.to_excel(writer, sheet_name="Deviation from Mean")

        with open(output_filename, "rb") as f:
            st.download_button("üì§ Download Excel File", f, file_name=output_filename)

    # ================================
    # üß© KMeans Clustering  *
    # Top 6 features most correlated with Grade
    # ================================
    elif analysis_type == "üß© KMeans Clustering Based on Selected Features":
        label_risk = "Grade de risque IWGDF"
        row_risk = df[df[0].astype(str).str.strip().str.lower() == label_risk.lower()]
        if row_risk.empty:
            st.error(f"Label '{label_risk}' not found.")
            st.stop()

        idx_risk = row_risk.index[0]
        risk_values = pd.to_numeric(df.iloc[idx_risk, 1:], errors='coerce')

        target_rows = {
            17: "Taille (m)", 18: "Poids (kg)", 35: "MESI PRESSION GO D", 36: "MESI PRESSION GO G",
            37: "IPS GO D", 38: "IPS GO G", 94: "AMPLI MTP1 D", 95: "AMPLI MTP1 G",
            96: "AMPLI TALO CRURALE D", 97: "AMPLI TALO CRURALE G", 108: "Pression MOYENNE DES MAX SESA D",
            109: "Pression MOYENNE DES MAX HALLUX D", 110: "Pression MOYENNE DES MAX  TM5 D",
            113: "Pression MOYENNE DES MAX  SESA G", 114: "Pression MOYENNE DES MAX  HALLUX G",
            115: "Pression MOYENNE DES MAX  TM5 G", 118: "DURO SESA D", 119: "DURO HALLUX D",
            120: "DURO TM5 D", 122: "DURO SESA G", 123: "DURO HALLUX G", 124: "DURO TM5 G",
            142: "√âpaisseur TOTALE PARTIES MOLLES  SESA D", 143: "√âpaisseur TOTALE PARTIES MOLLES  HALLUX D",
            144: "√âpaisseur TOTALE PARTIES MOLLES  TM5 D", 146: "√âpaisseur TOTALE PARTIES MOLLES  SESA G",
            147: "√âpaisseur TOTALE PARTIES MOLLES  HALLUX G", 148: "√âpaisseur TOTALE PARTIES MOLLES  TM5 G",
            150: "ROC SESA D", 151: "ROC HALLUX D", 152: "ROC TM5 D", 154: "ROC SESA G",
            155: "ROC HALLUX G", 156: "ROC TM5 G", 212: "SUDOSCAN main D", 213: "SUDOSCAN main G",
            214: "SUDOSCAN pied D", 215: "SUDOSCAN pied G"
        }

        # üì• Collect feature data
        feature_data = {}
        for row_index, label in target_rows.items():
            values = pd.to_numeric(df.iloc[row_index, 1:], errors='coerce')
            feature_data[label] = values

        features_df = pd.DataFrame(feature_data)

        # üìä Merge with Grade and clean
        full_df = features_df.copy()
        full_df["Grade"] = risk_values
        full_df = full_df.dropna()

        X = full_df.drop(columns=["Grade"])
        y = full_df["Grade"]

        # ‚öñÔ∏è Standardization
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)


        # üß† Clustering
        kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
        full_df["Cluster"] = kmeans.fit_predict(X_scaled)

        # üìà Evaluation Metrics
        silhouette = silhouette_score(X_scaled, full_df["Cluster"])
        ari = adjusted_rand_score(y, full_df["Cluster"])
        st.markdown(f"**Silhouette Score**: {silhouette:.3f} ‚Äî (1 = well-separated, 0 = overlap)<br>**Adjusted Rand Index (ARI)**: {ari:.3f} ‚Äî (1 = perfect match to true labels)", unsafe_allow_html=True)

        # ‚ú® Explanation Section
        with st.expander("‚ÑπÔ∏è Explanation of indicators and charts"):
            st.markdown("""
            - **Silhouette Score**: An index to measure the quality of clustering. A value close to 1 means that the clusters are well separated.
            - **Adjusted Rand Index (ARI)**: Shows the degree of alignment between the actual labels (Grade) and the clustering.
            - **PCA Visualization**: A 2D view of the data after dimensionality reduction with PCA to see the formation of clusters.
            """)
            
        # üìã Contingency Table
        cluster_vs_grade = pd.crosstab(full_df["Grade"], full_df["Cluster"])
        st.write("### üîç Cluster vs True Grade (Contingency Table)")
        st.dataframe(cluster_vs_grade)

        fig_kmeans, ax_kmeans = plt.subplots(figsize=(8, 6))
        sns.heatmap(cluster_vs_grade, annot=True, fmt="d", cmap="Blues", ax=ax_kmeans)
        ax_kmeans.set_title("Heatmap: Clusters vs True Grades")
        ax_kmeans.set_xlabel("Cluster")
        ax_kmeans.set_ylabel("True Grade")
        st.pyplot(fig_kmeans)

        # ‚ú® Explanation Section
        with st.expander("‚ÑπÔ∏è Explanation of indicators and charts"):
            st.markdown("""
            - **The heatmap shows that cluster 0 contains a mix of different true degrees, especially with 4 samples at degree 3.0 which is the most densely populated. 
            - **The other clusters (1, 2 and 3) have fewer samples and generally show limited dispersion. This is consistent with the previous consensus table and the low silhouette and ARI scores (0.097 and 0.101), indicating a low quality of cluster separation.
            """)
            
        # üìä Cluster Means
        st.subheader("üìä Moyennes par Cluster")
        cluster_means = full_df.groupby("Cluster").mean(numeric_only=True)
        st.dataframe(cluster_means.style.format(precision=2))
    
        # üåÄ PCA for Visualization
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_scaled)
        pca_df = pd.DataFrame(X_pca, columns=["PC1", "PC2"])
        pca_df["Cluster"] = full_df["Cluster"].values
        pca_df["Grade"] = y.values

        fig_pca, ax_pca = plt.subplots(figsize=(8, 6))
        sns.scatterplot(data=pca_df, x="PC1", y="PC2", hue="Cluster", style="Grade", palette="Set2", ax=ax_pca)
        ax_pca.set_title("2D PCA Visualization of Clusters and Grades")
        st.pyplot(fig_pca)

        # ‚ú® Explanation Section
        with st.expander("‚ÑπÔ∏è Explanation of indicators and charts"):
            st.markdown("""
            - **Horizontal axis (PC1): The first principal component that explains most of the variance in the data.
            - **Vertical axis (PC2): The second principal component that shows the remaining variance.
            - **The green dots (cluster 0) are scattered at the top and bottom of the plot, indicating the extent of this cluster in space.
            - **The orange dots (cluster 1) are located on the left and bottom and are limited in number.
            - **The blue dots (cluster 2) are seen in the middle and left and are less scattered.
            - **The pink dots (cluster 3) are concentrated at the top and center of the plot.
            - **The true degrees (0.0, 1.0, 2.0, 3.0) are represented by different symbols and do not seem to be well separated into clusters, which is consistent with the low Silhouette (0.097) and ARI (0.101) scores.

            Interpretation:
            The scatter of points and the overlapping of colors indicate that the obtained clustering did not separate the true degrees well. This is consistent with previous analyses (agreement table and heat map) and indicates the need to improve the clustering algorithm or the features used in PCA.
            """)

    # ================================
    # üß¨ ED Thickness & Hypodermis Analysis
    # ================================
    elif analysis_type == "üß¨ ED Thickness & Hypodermis Ultrasound Analysis":
        df_combined = prepare_data(df)

        selected_group = st.radio("Choose a group for intra-group analysis:", ["A (Grades 0-1)", "B (Grades 2-3)"])
        filtered = df_combined[df_combined["Group"] == selected_group]

        g1, g2 = (0, 1) if selected_group == "A (Grades 0-1)" else (2, 3)

        sub1 = filtered[filtered["Grade"] == g1]
        sub2 = filtered[filtered["Grade"] == g2]

        st.write(f"### üìã Data Overview - Group {selected_group}")
        st.dataframe(filtered)

        if sub1.empty or sub2.empty:
            st.warning(f"Not enough data to compare Grade {g1} and Grade {g2}.")
        else:
            st.subheader(f"üìä T-test: Grade {g1} vs Grade {g2}")
            ttest_results = []
            for col in filtered.columns[:-2]:
                t_stat, p_val = ttest_ind(sub1[col], sub2[col], equal_var=False, nan_policy='omit')
                ttest_results.append({"Variable": col, "p-value": p_val})
            ttest_df = pd.DataFrame(ttest_results).sort_values("p-value")
            st.dataframe(ttest_df.style.format({"p-value": "{:.4f}"}))

        st.subheader("üìà Correlation with Grade")
        if len(df_combined) >= 2:
            corr_with_grade = df_combined.drop(columns=["Group"]).corr()["Grade"].drop("Grade")
            corr_df = corr_with_grade.reset_index()
            corr_df.columns = ["Feature", "Correlation with Grade"]
            corr_df = corr_df.sort_values("Correlation with Grade", key=abs, ascending=False)
            st.dataframe(corr_df.style.format({"Correlation with Grade": "{:.2f}"}))

            fig_corr_grade, ax_corr_grade = plt.subplots(figsize=(10, 6))
            sns.barplot(data=corr_df, y="Feature", x="Correlation with Grade", palette="coolwarm", ax=ax_corr_grade)
            ax_corr_grade.set_title("Correlation of Each Feature with Grade")
            st.pyplot(fig_corr_grade)

        st.subheader("üìà Correlation Matrix")
        if len(filtered) >= 2:
            corr = filtered.drop(columns=["Grade", "Group"]).corr()
            fig_corr, ax_corr = plt.subplots(figsize=(12, 8))
            sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", ax=ax_corr)
            st.pyplot(fig_corr)

        st.subheader("üîÅ Linear Regression to Predict Grade")
        if len(filtered) >= 2:
            X = filtered.drop(columns=["Grade", "Group"])
            y = filtered["Grade"]

            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)

            model = LinearRegression()
            model.fit(X_scaled, y)

            coef_df = pd.DataFrame({"Feature": X.columns, "Coefficient": model.coef_})
            coef_df = coef_df.sort_values(by="Coefficient", key=abs, ascending=False)
            st.dataframe(coef_df.style.format({"Coefficient": "{:.4f}"}))

            fig_coef, ax_coef = plt.subplots(figsize=(10, 6))
            sns.barplot(data=coef_df, x="Coefficient", y="Feature", ax=ax_coef, palette="viridis")
            ax_coef.set_title("Feature Importance for Grade Prediction")
            st.pyplot(fig_coef)


    # ================================
    # üß© GMM Clustering vs Grades
    # Top 6 features most correlated with Grade
    # ================================
    elif analysis_type == "üß© GMM Clustering vs Grades":

        st.subheader("üîç GMM Clustering Based on Features Correlated with Grade")

        # üîπ Locate risk grades row
        label_risk = "Grade de risque IWGDF"
        row_risk = df[df[0].astype(str).str.strip().str.lower() == label_risk.lower()]
        if row_risk.empty:
            st.error(f"Label '{label_risk}' not found.")
            st.stop()

        idx_risk = row_risk.index[0]
        risk_values = pd.to_numeric(df.iloc[idx_risk, 1:], errors='coerce').dropna().astype(int)

        # üîπ Extract ED & Hypodermis data
        ed_data = df.iloc[126:134, 1:1 + len(risk_values)]
        ed_data.index = [
            "ED SESA R (mm)", "ED HALLUX R (mm)", "ED TM5 R (mm)", "ED Other R (mm)",
            "ED SESA L (mm)", "ED HALLUX L (mm)", "ED TM5 L (mm)", "ED Other L (mm)"
        ]
        df_ed = ed_data.T.apply(pd.to_numeric, errors='coerce')

        hypo_data = df.iloc[134:142, 1:1 + len(risk_values)]
        hypo_data.index = [
            "US Hypoderme SESA R (mm)", "US Hypoderme HALLUX R (mm)",
            "US Hypoderme TM5 R (mm)", "US Hypoderme Other R (mm)",
            "US Hypoderme SESA L (mm)", "US Hypoderme HALLUX L (mm)",
            "US Hypoderme TM5 L (mm)", "US Hypoderme Other L (mm)"
        ]
        df_hypo = hypo_data.T.apply(pd.to_numeric, errors='coerce')

        # üîπ Combine
        df_combined = pd.concat([df_ed, df_hypo], axis=1).dropna()
        df_combined["Grade"] = risk_values.loc[df_combined.index].values
        df_combined["Group"] = df_combined["Grade"].apply(lambda x: "A (Grades 0-1)" if x in [0, 1] else "B (Grades 2-3)")

        # üîπ Compute correlations with Grade
        corr_with_grade = df_combined.drop(columns=["Group"]).corr()["Grade"].drop("Grade")
        top_features = corr_with_grade.abs().sort_values(ascending=False).head(6).index.tolist()
        st.write("üìå Using top features:", ", ".join(top_features))

        # üîπ Clustering
        X = df_combined[top_features]
        y_true = df_combined["Grade"]

        from sklearn.preprocessing import StandardScaler
        from sklearn.mixture import GaussianMixture
        from sklearn.metrics import adjusted_rand_score
        from sklearn.decomposition import PCA

        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        gmm = GaussianMixture(n_components=4, random_state=42)
        clusters = gmm.fit_predict(X_scaled)
        df_combined["GMM_Cluster"] = clusters

        # üîπ Evaluation Table
        cluster_vs_grade = pd.crosstab(df_combined["Grade"], df_combined["GMM_Cluster"])
        st.write("### üîÅ Cluster vs True Grade (Contingency Table)")
        st.dataframe(cluster_vs_grade)

        fig_heatmap, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(cluster_vs_grade, annot=True, fmt="d", cmap="Blues", ax=ax)
        ax.set_title("GMM Clustering vs Grade")
        ax.set_xlabel("GMM Cluster")
        ax.set_ylabel("True Grade")
        st.pyplot(fig_heatmap)

        # üîπ ARI
        ari = adjusted_rand_score(y_true, clusters)
        st.metric("üßÆ Adjusted Rand Index (ARI)", f"{ari:.3f}")

        # üîπ PCA Visualization
        X_pca = PCA(n_components=2).fit_transform(X_scaled)
        pca_df = pd.DataFrame(X_pca, columns=["PC1", "PC2"])
        pca_df["GMM_Cluster"] = clusters
        pca_df["True Grade"] = y_true.values

        fig_pca, ax_pca = plt.subplots(figsize=(8, 6))
        sns.scatterplot(data=pca_df, x="PC1", y="PC2", hue="GMM_Cluster", style="True Grade", palette="deep", ax=ax_pca)
        ax_pca.set_title("PCA View: GMM Clusters vs True Grades")
        st.pyplot(fig_pca)

    
    # ================================
    # üß† Correlation Between Key Parameters
    # ================================
    elif analysis_type == "üß† Correlation Between Key Parameters":
        st.subheader("üîó Correlation Between Selected DIAFOOT Parameters")

        target_rows = {
            17: "Taille (m)", 18: "Poids (kg)", 35: "MESI PRESSION GO D", 36: "MESI PRESSION GO G",
            37: "IPS GO D", 38: "IPS GO G", 94: "AMPLI MTP1 D", 95: "AMPLI MTP1 G",
            96: "AMPLI TALO CRURALE D", 97: "AMPLI TALO CRURALE G",
            108: "Pression MOYENNE DES MAX SESA D", 109: "Pression MOYENNE DES MAX HALLUX D",
            110: "Pression MOYENNE DES MAX  TM5 D", 113: "Pression MOYENNE DES MAX  SESA G",
            114: "Pression MOYENNE DES MAX  HALLUX G", 115: "Pression MOYENNE DES MAX  TM5 G",
            118: "DURO SESA D", 119: "DURO HALLUX D", 120: "DURO TM5 D", 122: "DURO SESA G",
            123: "DURO HALLUX G", 124: "DURO TM5 G", 142: "√âpaisseur TOTALE PARTIES MOLLES  SESA D",
            143: "√âpaisseur TOTALE PARTIES MOLLES  HALLUX D", 144: "√âpaisseur TOTALE PARTIES MOLLES  TM5 D",
            146: "√âpaisseur TOTALE PARTIES MOLLES  SESA G", 147: "√âpaisseur TOTALE PARTIES MOLLES  HALLUX G",
            148: "√âpaisseur TOTALE PARTIES MOLLES  TM5 G", 150: "ROC SESA D", 151: "ROC HALLUX D",
            152: "ROC TM5 D", 154: "ROC SESA G", 155: "ROC HALLUX G", 156: "ROC TM5 G",
            212: "SUDOSCAN main D", 213: "SUDOSCAN main G", 214: "SUDOSCAN pied D", 215: "SUDOSCAN pied G"
        }

        data_dict = {}
        for row_index, label in target_rows.items():
            values = pd.to_numeric(df.iloc[row_index, 1:], errors='coerce')
            data_dict[label] = values

        df_corr = pd.DataFrame(data_dict).dropna()

        st.write("### üßæ Cleaned Data Table (dropna)")
        st.dataframe(df_corr)

        # Correlation matrix
        corr_matrix = df_corr.corr()

        st.write("### üìä Correlation Matrix")
        cmap = "coolwarm"
        st.dataframe(corr_matrix.style.background_gradient(cmap=cmap, axis=None).format("{:.2f}"))

        fig_corr, ax = plt.subplots(figsize=(14, 12))
        sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5, ax=ax)
        ax.set_title("Correlation Heatmap of Selected Parameters", fontsize=16)
        st.pyplot(fig_corr)

        # Optional: correlation with a specific variable
        selected_ref = st.selectbox("üìå Show correlations with a specific variable:", corr_matrix.columns.tolist())
        ref_corr = corr_matrix[selected_ref].drop(labels=[selected_ref])
        ref_corr_sorted = ref_corr.reindex(ref_corr.abs().sort_values(ascending=False).index)

        fig_bar, ax_bar = plt.subplots(figsize=(10, 6))
        sns.barplot(y=ref_corr_sorted.index, x=ref_corr_sorted.values, palette="vlag", ax=ax_bar)
        ax_bar.set_title(f"Correlation of Other Variables with '{selected_ref}'")
        ax_bar.set_xlabel("Correlation Coefficient")
        ax_bar.set_xlim(-1, 1)
        st.pyplot(fig_bar)

    # ================================
    # üìå Mechanical Features Correlation Analysis
    # ================================
    elif analysis_type == "üìå Mechanical Features Correlation Analysis":

        target_rows = {
            17: "Taille (m)", 18: "Poids (kg)", 35: "MESI PRESSION GO D", 36: "MESI PRESSION GO G",
            37: "IPS GO D", 38: "IPS GO G", 94: "AMPLI MTP1 D", 95: "AMPLI MTP1 G",
            96: "AMPLI TALO CRURALE D", 97: "AMPLI TALO CRURALE G",
            108: "Pression MOYENNE DES MAX SESA D", 109: "Pression MOYENNE DES MAX HALLUX D",
            110: "Pression MOYENNE DES MAX  TM5 D", 113: "Pression MOYENNE DES MAX  SESA G",
            114: "Pression MOYENNE DES MAX  HALLUX G", 115: "Pression MOYENNE DES MAX  TM5 G",
            118: "DURO SESA D", 119: "DURO HALLUX D", 120: "DURO TM5 D", 122: "DURO SESA G",
            123: "DURO HALLUX G", 124: "DURO TM5 G", 142: "√âpaisseur TOTALE PARTIES MOLLES  SESA D",
            143: "√âpaisseur TOTALE PARTIES MOLLES  HALLUX D", 144: "√âpaisseur TOTALE PARTIES MOLLES  TM5 D",
            146: "√âpaisseur TOTALE PARTIES MOLLES  SESA G", 147: "√âpaisseur TOTALE PARTIES MOLLES  HALLUX G",
            148: "√âpaisseur TOTALE PARTIES MOLLES  TM5 G", 150: "ROC SESA D", 151: "ROC HALLUX D",
            152: "ROC TM5 D", 154: "ROC SESA G", 155: "ROC HALLUX G", 156: "ROC TM5 G",
            212: "SUDOSCAN main D", 213: "SUDOSCAN main G", 214: "SUDOSCAN pied D", 215: "SUDOSCAN pied G"
        }
        st.subheader("üî¨ Analyse de corr√©lation entre les param√®tres m√©caniques")

        data = {}
        labels = []

        for row_idx, label in target_rows.items():
            values = pd.to_numeric(df.iloc[row_idx, 1:], errors="coerce")
            if values.isna().sum() < len(values): 
                data[label] = values
                labels.append(label)

        mech_df = pd.DataFrame(data)

        if mech_df.empty or len(mech_df) < 2:
            st.warning("Pas assez de donn√©es valides pour cette analyse.")
        else:
            st.write("### üßæ Donn√©es disponibles")
            st.dataframe(mech_df)

            corr = mech_df.corr()

            st.write("### üîó Matrice de corr√©lation")
            fig_corr, ax_corr = plt.subplots(figsize=(16, 10))
            sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", ax=ax_corr, mask=np.triu(np.ones_like(corr, dtype=bool)))
            st.pyplot(fig_corr)

            threshold = st.slider("Seuil de corr√©lation √©lev√©e", 0.5, 1.0, 0.8, 0.05)
            high_corr_pairs = []

            for i in range(len(corr.columns)):
                for j in range(i + 1, len(corr.columns)):
                    val = corr.iloc[i, j]
                    if abs(val) >= threshold:
                        high_corr_pairs.append({
                            "Feature 1": corr.columns[i],
                            "Feature 2": corr.columns[j],
                            "Correlation": round(val, 3)
                        })

            if high_corr_pairs:
                st.write(f"### üö® Paires avec corr√©lation absolue > {threshold}")
                st.dataframe(pd.DataFrame(high_corr_pairs))
            else:
                st.success("Aucune paire avec corr√©lation √©lev√©e selon le seuil d√©fini.")

            st.markdown("üí° *Vous pouvez envisager de supprimer l'une des variables de chaque paire tr√®s corr√©l√©e pour r√©duire la redondance.*")

# ================================
# üìé File Not Uploaded Message
# ================================
else:
    st.info("Please upload an Excel file containing the 'DIAFOOT' sheet.")
